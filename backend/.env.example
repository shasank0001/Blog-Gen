# ============================================
# Content Strategist Agent - Environment Config
# ============================================
# Copy this file to .env and fill in your API keys
#
# QUICK REFERENCE:
# ----------------
# âœ… MANDATORY: DATABASE_URL, SECRET_KEY, PINECONE_API_KEY, FIRECRAWL_API_KEY
# âœ… MANDATORY: At least ONE of: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, or Ollama
# ðŸ”§ OPTIONAL:  Additional LLM providers (Anthropic, Google)
# ðŸ”§ OPTIONAL:  Ollama settings (only if you want local/private inference)

PROJECT_NAME="Content Strategist Agent"

# ============================================
# âœ… MANDATORY: LLM PROVIDER (Need at least ONE)
# ============================================
# You must configure AT LEAST ONE of these LLM providers.
# If you don't want to use cloud providers, set up Ollama instead (see bottom).

# OpenAI API Key
# Get yours at: https://platform.openai.com/api-keys
# 1. Sign up/login at https://platform.openai.com
# 2. Go to API Keys section
# 3. Click "Create new secret key"
OPENAI_API_KEY="sk-your-openai-key"

# ============================================
# âœ… MANDATORY: EXTERNAL SERVICES
# ============================================

# Firecrawl API Key (Required for web research)
# Get yours at: https://www.firecrawl.dev/
# 1. Sign up at https://www.firecrawl.dev/
# 2. Go to Dashboard > API Keys
# 3. Copy your API key (starts with 'fc-')
FIRECRAWL_API_KEY="fc-your-firecrawl-key"

# Pinecone API Key (Required for vector storage)
# Get yours at: https://www.pinecone.io/
# 1. Sign up at https://www.pinecone.io/
# 2. Create a new project (free tier available)
# 3. Go to API Keys in your project dashboard
# 4. Copy the API key (starts with 'pcsk_')
PINECONE_API_KEY="pcsk-your-pinecone-key"
PINECONE_ENV="us-east-1"
PINECONE_INDEX_NAME="content-strategist"

# ============================================
# âœ… MANDATORY: DATABASE & AUTH
# ============================================

# Database URL (Default works with Docker Compose)
DATABASE_URL="postgresql+asyncpg://user:password@localhost:5432/blog_gen"

# JWT Secret Key - CHANGE THIS!
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
SECRET_KEY="replace-with-strong-secret"
ACCESS_TOKEN_EXPIRE_MINUTES=30

# ============================================
# ðŸ”§ OPTIONAL: ADDITIONAL LLM PROVIDERS
# ============================================
# Only fill these if you want to use Claude or Gemini models.
# Leave empty if you're only using OpenAI or Ollama.

# Anthropic API Key (enables Claude models)
# Get yours at: https://console.anthropic.com/
ANTHROPIC_API_KEY=""

# Google AI API Key (enables Gemini models)
# Get yours at: https://aistudio.google.com/apikey
GOOGLE_API_KEY=""

# ============================================
# ðŸ”§ OPTIONAL: LOCAL INFERENCE (Ollama)
# ============================================
# SKIP THIS SECTION if you're using cloud providers (OpenAI/Anthropic/Google).
# Only configure if you want to run LLMs locally for privacy.
#
# Setup required:
# 1. Install Ollama: https://ollama.com/download
# 2. Pull a model: ollama pull gemma3:4b
# 3. Start Ollama: OLLAMA_HOST=0.0.0.0 ollama serve (Linux)

# Set to 'true' to force ALL LLM calls through local Ollama
USE_LOCAL_LLM=false

# Set to 'true' to use local embeddings (requires Ollama + nomic-embed-text)
USE_LOCAL_EMBEDDINGS=false

# Ollama server URL
# For local dev (not Docker): http://localhost:11434
# For Docker on Linux/Mac: http://host.docker.internal:11434
OLLAMA_BASE_URL="http://host.docker.internal:11434"

# Which Ollama model to use (must be pulled first)
OLLAMA_MODEL="gemma3:4b" # this a small model so it may not handel large blogs make sure to set a higer context window in the ollama server than default which may be around 4k

# Local embedding model (only if USE_LOCAL_EMBEDDINGS=true)
LOCAL_EMBEDDING_MODEL="nomic-embed-text"
